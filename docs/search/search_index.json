{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Engineer Portal \u00b6 Welcome to the Navigator Engineering Portal, a dedicated hub for our internal development and operations teams . This portal serves as a central repository of knowledge, resources, and tools essential for the effective development, deployment, and maintenance of Navigator. Here, you will find in-depth technical documentation, development guidelines, release management protocols, and much more. This portal is designed to foster collaboration, knowledge sharing, and innovation within our team, ensuring that we collectively drive Navigator towards excellence. Stay updated with the latest development practices, explore detailed documentation, and engage with your fellow engineers to make Navigator a benchmark in its domain.","title":"Home"},{"location":"#welcome-to-the-engineer-portal","text":"Welcome to the Navigator Engineering Portal, a dedicated hub for our internal development and operations teams . This portal serves as a central repository of knowledge, resources, and tools essential for the effective development, deployment, and maintenance of Navigator. Here, you will find in-depth technical documentation, development guidelines, release management protocols, and much more. This portal is designed to foster collaboration, knowledge sharing, and innovation within our team, ensuring that we collectively drive Navigator towards excellence. Stay updated with the latest development practices, explore detailed documentation, and engage with your fellow engineers to make Navigator a benchmark in its domain.","title":"Welcome to the Engineer Portal"},{"location":"engineering/architecture/","text":"Architecture \u00b6 In our continuous effort to enhance clarity and understanding of Navigator's architecture, we have adopted the C4 model as our primary framework for architectural visualization and documentation. The C4 model, conceptualized by Simon Brown, offers a comprehensive approach to describing software architecture, breaking it down into four distinct levels: Context, Containers, Components, and Code. C1 - Context \u00b6 C2 - Containers \u00b6 C3 - Components \u00b6","title":"Architecture"},{"location":"engineering/architecture/#architecture","text":"In our continuous effort to enhance clarity and understanding of Navigator's architecture, we have adopted the C4 model as our primary framework for architectural visualization and documentation. The C4 model, conceptualized by Simon Brown, offers a comprehensive approach to describing software architecture, breaking it down into four distinct levels: Context, Containers, Components, and Code.","title":"Architecture"},{"location":"engineering/architecture/#c1-context","text":"","title":"C1 - Context"},{"location":"engineering/architecture/#c2-containers","text":"","title":"C2 - Containers"},{"location":"engineering/architecture/#c3-components","text":"","title":"C3 - Components"},{"location":"engineering/cloudwatch/","text":"CONSULTING LOGS IN CLOUDWATCH \u00b6 In Cloudwatch, use these Fluent queries for log extraction. For a complete overview of Cloudwatch, review the documentation: here Steps to perform the queries: 1. Go to AWS Cloudwatch. 2. Select the N. Virginia region. 3. Select Logs > Logs Insights. 4. Select the log group: fluent-bit-cloudwatch. Log Extraction: \u00b6 You can consult the endpoint by changing the @logStream for the required application or API, for example: dataintegrator-worker-production for Workers logs. navigator-api-production for Navapi log queries. dataintegrator-production for scheduler-related logs. Example: fields @message | sort @timestamp asc | limit 10000 | filter @logStream like /dataintegrator-worker-production/ | filter @message like /Task Consumed/ Verifying Logs: \u00b6 fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production/ | limit 10000 or fields @message | sort @timestamp asc | limit 10000 | filter @logStream like /dataintegrator-worker-production/ | filter @message like /Task Consumed/ This allows us to see the tasks consumed (fully executed) by the workers. fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production-*/ | filter @message like /troc.worked_hours/ | limit 20 fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production/ | filter @message like /deals_dormant/ | limit 20 To validate specific execution time, filter by @timestamp: fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production/ | filter @timestamp >= 1698409500798 and @timestamp <= 1698409500898 | limit 300","title":"CloudWatch"},{"location":"engineering/cloudwatch/#consulting-logs-in-cloudwatch","text":"In Cloudwatch, use these Fluent queries for log extraction. For a complete overview of Cloudwatch, review the documentation: here Steps to perform the queries: 1. Go to AWS Cloudwatch. 2. Select the N. Virginia region. 3. Select Logs > Logs Insights. 4. Select the log group: fluent-bit-cloudwatch.","title":"CONSULTING LOGS IN CLOUDWATCH"},{"location":"engineering/cloudwatch/#log-extraction","text":"You can consult the endpoint by changing the @logStream for the required application or API, for example: dataintegrator-worker-production for Workers logs. navigator-api-production for Navapi log queries. dataintegrator-production for scheduler-related logs. Example: fields @message | sort @timestamp asc | limit 10000 | filter @logStream like /dataintegrator-worker-production/ | filter @message like /Task Consumed/","title":"Log Extraction:"},{"location":"engineering/cloudwatch/#verifying-logs","text":"fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production/ | limit 10000 or fields @message | sort @timestamp asc | limit 10000 | filter @logStream like /dataintegrator-worker-production/ | filter @message like /Task Consumed/ This allows us to see the tasks consumed (fully executed) by the workers. fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production-*/ | filter @message like /troc.worked_hours/ | limit 20 fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production/ | filter @message like /deals_dormant/ | limit 20 To validate specific execution time, filter by @timestamp: fields @timestamp, @message, @logStream, @log | sort @timestamp desc | filter @logStream like /dataintegrator-worker-production/ | filter @timestamp >= 1698409500798 and @timestamp <= 1698409500898 | limit 300","title":"Verifying Logs:"},{"location":"engineering/development-guidelines/","text":"Development Guidelines \u00b6 add some comments here title \u00b6","title":"Development Guidelines"},{"location":"engineering/development-guidelines/#development-guidelines","text":"add some comments here","title":"Development Guidelines"},{"location":"engineering/development-guidelines/#title","text":"","title":"title"},{"location":"engineering/environments/","text":"Environments \u00b6","title":"Environments"},{"location":"engineering/environments/#environments","text":"","title":"Environments"},{"location":"engineering/stats_pg/","text":"Sticky Bits Installation and Configuration Guide \u00b6 Introduction to Sticky Bits \u00b6 Sticky Bits is a special type of file permission in Unix and Linux systems. When the sticky bit is set on a directory, it means that only the file's owner, the directory's owner, or the root user can rename, move, or delete the contained files, regardless of the file's individual permissions. This feature is particularly useful for directories like /tmp , where multiple users have permission to write, but should not be able to delete or rename each other's files. Prerequisites: \u00b6 Ensure gcc is installed. You can install it using the following command: bash sudo apt install gcc Install acl using the command below: bash sudo apt-get install acl Create the dataintegrator group and associate it with users: Creating the group: bash sudo groupadd dataintegrator Verify that the group has been created: bash grep 'dataintegrator' /etc/group Associate users with the group: bash sudo usermod -aG dataintegrator user Create the dataintegrator user and associate it with the group: bash sudo usermod -aG dataintegrator dataintegrator Create a folder in the directory: bash mkdir /home/ubuntu/symbist/ To assign permissions: sudo setfacl -Rdm g:dataintegrator:rx /home/ubuntu/symbits/ sudo chgrp dataintegrator /home/ubuntu/symbits/ -fR sudo chmod g+s /home/ubuntu/symbits -fR Apply permissions to the directory: chmod 775 /home/ubuntu/symbits Ensure to follow these steps carefully for the successful installation and configuration of Sticky Bits.","title":"Sticky Bits Installation and Configuration Guide"},{"location":"engineering/stats_pg/#sticky-bits-installation-and-configuration-guide","text":"","title":"Sticky Bits Installation and Configuration Guide"},{"location":"engineering/stats_pg/#introduction-to-sticky-bits","text":"Sticky Bits is a special type of file permission in Unix and Linux systems. When the sticky bit is set on a directory, it means that only the file's owner, the directory's owner, or the root user can rename, move, or delete the contained files, regardless of the file's individual permissions. This feature is particularly useful for directories like /tmp , where multiple users have permission to write, but should not be able to delete or rename each other's files.","title":"Introduction to Sticky Bits"},{"location":"engineering/stats_pg/#prerequisites","text":"Ensure gcc is installed. You can install it using the following command: bash sudo apt install gcc Install acl using the command below: bash sudo apt-get install acl Create the dataintegrator group and associate it with users: Creating the group: bash sudo groupadd dataintegrator Verify that the group has been created: bash grep 'dataintegrator' /etc/group Associate users with the group: bash sudo usermod -aG dataintegrator user Create the dataintegrator user and associate it with the group: bash sudo usermod -aG dataintegrator dataintegrator Create a folder in the directory: bash mkdir /home/ubuntu/symbist/ To assign permissions: sudo setfacl -Rdm g:dataintegrator:rx /home/ubuntu/symbits/ sudo chgrp dataintegrator /home/ubuntu/symbits/ -fR sudo chmod g+s /home/ubuntu/symbits -fR Apply permissions to the directory: chmod 775 /home/ubuntu/symbits Ensure to follow these steps carefully for the successful installation and configuration of Sticky Bits.","title":"Prerequisites:"},{"location":"engineering/stern_use/","text":"# Stern Usage Guide for Observing Logs in Kubernetes - Navigator Cluster This guide provides detailed instructions on how to use the `stern` tool for observing and filtering pod logs in a Kubernetes cluster, particularly for tracking services like `dataintegrator` and `navigator-api` across different environments (production, staging, development). ## Prerequisites - `stern` installed on your machine. - Access to a Kubernetes cluster. - Configured `kubectl` and access to the Kubernetes cluster where the services are running. ## Basic Log Observation ### In Production To observe logs from `dataintegrator-production-*` in the `production` namespace: ```bash ./stern -n production dataintegrator-production-* To observe logs from dataintegrator-worker-production-* in production : ./stern -n production dataintegrator-worker-production-* To observe logs from navigator-api-production-* in production : ./stern -n production navigator-api-production-* In Staging \u00b6 To observe logs from dataintegrator-production-* in the staging namespace: ./stern -n staging dataintegrator-production-* In Development \u00b6 To observe logs from dataintegrator-production-* in the dev namespace: ./stern -n dev dataintegrator-production-* Log Filtering \u00b6 To filter and validate a specific log, you can use grep . For example, to filter logs from navigator-api-production-* in production for a specific validation: ./stern -n production navigator-api-production-* | grep \"specific_validation\" Replace \"specific_validation\" with the specific text string you are looking for in the logs. Additional Tips \u00b6 Regular Expressions : stern supports the use of regular expressions to specify pods, allowing great flexibility in log selection. Real-time Logging : stern displays logs in real-time, which is useful for live debugging and monitoring. Log History : By default, stern retrieves the last 100 lines of logs. You can change this behavior with the --tail flag. Stern Help : For more options and assistance, run stern --help . This guide should provide you with a clear understanding of how to use stern for monitoring and filtering logs in a Kubernetes environment. Be sure to adapt the commands and filters to the specific needs of your environment and applications. ```","title":"Stern"},{"location":"engineering/stern_use/#in-staging","text":"To observe logs from dataintegrator-production-* in the staging namespace: ./stern -n staging dataintegrator-production-*","title":"In Staging"},{"location":"engineering/stern_use/#in-development","text":"To observe logs from dataintegrator-production-* in the dev namespace: ./stern -n dev dataintegrator-production-*","title":"In Development"},{"location":"engineering/stern_use/#log-filtering","text":"To filter and validate a specific log, you can use grep . For example, to filter logs from navigator-api-production-* in production for a specific validation: ./stern -n production navigator-api-production-* | grep \"specific_validation\" Replace \"specific_validation\" with the specific text string you are looking for in the logs.","title":"Log Filtering"},{"location":"engineering/stern_use/#additional-tips","text":"Regular Expressions : stern supports the use of regular expressions to specify pods, allowing great flexibility in log selection. Real-time Logging : stern displays logs in real-time, which is useful for live debugging and monitoring. Log History : By default, stern retrieves the last 100 lines of logs. You can change this behavior with the --tail flag. Stern Help : For more options and assistance, run stern --help . This guide should provide you with a clear understanding of how to use stern for monitoring and filtering logs in a Kubernetes environment. Be sure to adapt the commands and filters to the specific needs of your environment and applications. ```","title":"Additional Tips"},{"location":"engineering/sticky-bits/","text":"Sticky Bits Installation and Configuration Guide \u00b6 Introduction to Sticky Bits \u00b6 Sticky Bits is a special type of file permission in Unix and Linux systems. When the sticky bit is set on a directory, it means that only the file's owner, the directory's owner, or the root user can rename, move, or delete the contained files, regardless of the file's individual permissions. This feature is particularly useful for directories like /tmp , where multiple users have permission to write, but should not be able to delete or rename each other's files. Prerequisites: \u00b6 Ensure gcc is installed. You can install it using the following command: bash sudo apt install gcc Install acl using the command below: bash sudo apt-get install acl Create the dataintegrator group and associate it with users: Creating the group: bash sudo groupadd dataintegrator Verify that the group has been created: bash grep 'dataintegrator' /etc/group Associate users with the group: bash sudo usermod -aG dataintegrator user Create the dataintegrator user and associate it with the group: bash sudo usermod -aG dataintegrator dataintegrator Create a folder in the directory: bash mkdir /home/ubuntu/symbist/ To assign permissions: sudo setfacl -Rdm g:dataintegrator:rx /home/ubuntu/symbits/ sudo chgrp dataintegrator /home/ubuntu/symbits/ -fR sudo chmod g+s /home/ubuntu/symbits -fR Apply permissions to the directory: chmod 775 /home/ubuntu/symbits Ensure to follow these steps carefully for the successful installation and configuration of Sticky Bits.","title":"Sticky Bits"},{"location":"engineering/sticky-bits/#sticky-bits-installation-and-configuration-guide","text":"","title":"Sticky Bits Installation and Configuration Guide"},{"location":"engineering/sticky-bits/#introduction-to-sticky-bits","text":"Sticky Bits is a special type of file permission in Unix and Linux systems. When the sticky bit is set on a directory, it means that only the file's owner, the directory's owner, or the root user can rename, move, or delete the contained files, regardless of the file's individual permissions. This feature is particularly useful for directories like /tmp , where multiple users have permission to write, but should not be able to delete or rename each other's files.","title":"Introduction to Sticky Bits"},{"location":"engineering/sticky-bits/#prerequisites","text":"Ensure gcc is installed. You can install it using the following command: bash sudo apt install gcc Install acl using the command below: bash sudo apt-get install acl Create the dataintegrator group and associate it with users: Creating the group: bash sudo groupadd dataintegrator Verify that the group has been created: bash grep 'dataintegrator' /etc/group Associate users with the group: bash sudo usermod -aG dataintegrator user Create the dataintegrator user and associate it with the group: bash sudo usermod -aG dataintegrator dataintegrator Create a folder in the directory: bash mkdir /home/ubuntu/symbist/ To assign permissions: sudo setfacl -Rdm g:dataintegrator:rx /home/ubuntu/symbits/ sudo chgrp dataintegrator /home/ubuntu/symbits/ -fR sudo chmod g+s /home/ubuntu/symbits -fR Apply permissions to the directory: chmod 775 /home/ubuntu/symbits Ensure to follow these steps carefully for the successful installation and configuration of Sticky Bits.","title":"Prerequisites:"},{"location":"engineering/aws/","text":"AWS \u00b6","title":"Home"},{"location":"engineering/aws/#aws","text":"","title":"AWS"},{"location":"engineering/aws/access/","text":"### AWSCLI Installation We execute the installation: ```bash curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Configuration: \u00b6 Format of Configuration and Credentials Files \u00b6 The config and credentials files are organized into sections. Sections include profiles, sso-sessions , and services. A section is a named collection of configurations and continues until another section definition line is found. Multiple profiles and sections can be stored in the config and credentials files. They are plain text files that use the following format: Section names appear in brackets [ ] such as [default] , [profile user1] , and [sso-session] . All entries in a section adopt the general format of setting_name=value . Lines can be commented out if they start with a hash character (#) . The config and credentials files contain the following types of sections: We configure the config and credentials files in the ./aws folder - In Credentials [default] aws_access_key_id = aws_secret_access_key = [tr-dev] role_arn = source_profile = [tr-prod] role_arn = source_profile = [trocglobal] region = AWS_ACCESS_KEY_ID= AWS_SECRET_ACCESS_KEY= mfa_serial= Before installing AWSUME, we install: \u00b6 Installing pyenv: \u00b6 We install the pyenv program. curl https://pyenv.run | bash Execute the following command to export the configuration to .bashrc : echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init --path)\"' >> ~/.bashrc Once pyenv is installed, we proceed to verify if it is working correctly with the following commands: pyenv install --list # Verify available versions pyenv install 3.10.12 # Select a version to install Within the navigator directory, activate the virtual environment env: Create the Python virtual environment: pyenv local 3.10.12 python -V Then, activate the virtual environment: python -m venv .venv source .venv/bin/activate AWSUME Installation \u00b6 pip install awsume Alias \u00b6 The alias is necessary for Unix-type systems and shells, including Bash and Zsh. It is not necessary for Windows-type shells like PowerShell or Windows Command Prompt. The alias should be defined in your shell's startup file, so it loads in every shell session. It should look something like this: We adjust the awsume alias to be detected from pyenv. alias awsume=\". $(pyenv which awsume)\" AWSUME will take the configuration from .aws/ and the credentials, and we will be able to execute aws commands. For connecting to dev --> awsume tr-dev - We will be prompted for our MFA code and logged into Amazon. For production --> awsume tr-prod Note: to switch environments, an update of the aws regions is required, for that we can apply the following command: aws eks --region us-east-2 update-kubeconfig --name trocglobal-services This Markdown manual provides a step-by-step guide for installing AWSCLI and AWSUME, including configuring pyenv and managing Python virtual environments. ```","title":"Access"},{"location":"engineering/aws/access/#configuration","text":"","title":"Configuration:"},{"location":"engineering/aws/access/#format-of-configuration-and-credentials-files","text":"The config and credentials files are organized into sections. Sections include profiles, sso-sessions , and services. A section is a named collection of configurations and continues until another section definition line is found. Multiple profiles and sections can be stored in the config and credentials files. They are plain text files that use the following format: Section names appear in brackets [ ] such as [default] , [profile user1] , and [sso-session] . All entries in a section adopt the general format of setting_name=value . Lines can be commented out if they start with a hash character (#) . The config and credentials files contain the following types of sections: We configure the config and credentials files in the ./aws folder - In Credentials [default] aws_access_key_id = aws_secret_access_key = [tr-dev] role_arn = source_profile = [tr-prod] role_arn = source_profile = [trocglobal] region = AWS_ACCESS_KEY_ID= AWS_SECRET_ACCESS_KEY= mfa_serial=","title":"Format of Configuration and Credentials Files"},{"location":"engineering/aws/access/#before-installing-awsume-we-install","text":"","title":"Before installing AWSUME, we install:"},{"location":"engineering/aws/access/#installing-pyenv","text":"We install the pyenv program. curl https://pyenv.run | bash Execute the following command to export the configuration to .bashrc : echo 'export PYENV_ROOT=\"$HOME/.pyenv\"' >> ~/.bashrc echo 'export PATH=\"$PYENV_ROOT/bin:$PATH\"' >> ~/.bashrc echo 'eval \"$(pyenv init --path)\"' >> ~/.bashrc Once pyenv is installed, we proceed to verify if it is working correctly with the following commands: pyenv install --list # Verify available versions pyenv install 3.10.12 # Select a version to install Within the navigator directory, activate the virtual environment env: Create the Python virtual environment: pyenv local 3.10.12 python -V Then, activate the virtual environment: python -m venv .venv source .venv/bin/activate","title":"Installing pyenv:"},{"location":"engineering/aws/access/#awsume-installation","text":"pip install awsume","title":"AWSUME Installation"},{"location":"engineering/aws/access/#alias","text":"The alias is necessary for Unix-type systems and shells, including Bash and Zsh. It is not necessary for Windows-type shells like PowerShell or Windows Command Prompt. The alias should be defined in your shell's startup file, so it loads in every shell session. It should look something like this: We adjust the awsume alias to be detected from pyenv. alias awsume=\". $(pyenv which awsume)\" AWSUME will take the configuration from .aws/ and the credentials, and we will be able to execute aws commands. For connecting to dev --> awsume tr-dev - We will be prompted for our MFA code and logged into Amazon. For production --> awsume tr-prod Note: to switch environments, an update of the aws regions is required, for that we can apply the following command: aws eks --region us-east-2 update-kubeconfig --name trocglobal-services This Markdown manual provides a step-by-step guide for installing AWSCLI and AWSUME, including configuring pyenv and managing Python virtual environments. ```","title":"Alias"},{"location":"engineering/aws/openvpn/","text":"Installing the OpenVPN Connect Client on Windows \u00b6 Download the Installer \u00b6 Visit the download link provided: OpenVPN Connect v3 . Click on \"Download OpenVPN Connect v3\". Run the Installer \u00b6 Locate the .msi file you downloaded and double-click it to start the installation process. Installation Assistant \u00b6 Follow the instructions in the installation wizard. This usually includes agreeing to the terms of the license agreement and selecting the destination folder for installation. Finish Installation \u00b6 Once the wizard is finished, click \"Finish\" to complete the installation. More details available at OpenVPN Connect Client for Windows . Using the OpenVPN Connect Client to connect to a VPN \u00b6 Launch OpenVPN Connect Client \u00b6 Look for OpenVPN Connect in the Windows Start menu and open it. Import VPN Profile \u00b6 On the main interface, click on \"Import Profile\". You have two options: URL : Enter the URL provided by your VPN administrator. FILE : Drag and drop the .ovpn file into the window or click \"BROWSE\" to locate and select the file on your computer. Note: Import the .ovpn files generated for Navigator's DEV and PROD connections here. Connect to the VPN \u00b6 Enter your username and password provided by the VPN administrator. Click \"Connect\" to establish the connection to the VPN server. Connection Verification \u00b6 Verify that the connection is successful. Using the VPN \u00b6 With the VPN connected, all your internet activities will go through the VPN. To disconnect, open OpenVPN Connect and click Disconnect. Profile Management \u00b6 Manage multiple VPN profiles within the app. To switch between profiles, disconnect from the current VPN and repeat the connection process with the desired profile. Getting Started with OpenVPN 3 Linux \u00b6 OpenVPN 3 Linux is a modern client using the OpenVPN 3 Core Library. It enables unprivileged users to initiate and manage VPN sessions through D-Bus, enhancing privilege separation. This client is optimized for contemporary Linux distributions, but it's compatible with any platform that supports D-Bus. Pre-built Packages \u00b6 OpenVPN 3 Linux v21 offers pre-built packages for various distributions. For production environments, opt for stable releases. Development or beta releases provide more frequent updates. Supported Distributions \u00b6 Debian (Buster, Bullseye, Bookworm) Fedora (37, 38, Rawhide) Red Hat Enterprise Linux / CentOS (7, 8, 9) Ubuntu LTS (Focal, Jammy) Ubuntu (Kinetic, Lunar, Mantic) Installation Instructions \u00b6 1. Add the Repository: \u00b6 For Debian / Ubuntu, start by installing necessary packages: sudo apt install apt-transport-https curl 2. Retrieve and Add the Package Signing Key: \u00b6 Create a directory for the keyrings, if needed: mkdir -p /etc/apt/keyrings Download and add the OpenVPN Inc package signing key: curl -sSfL https://packages.openvpn.net/packages-repo.gpg > /etc/apt/keyrings/openvpn.asc Add the OpenVPN repository to your system's source list: echo \"deb [signed-by=/etc/apt/keyrings/openvpn.asc] https://packages.openvpn.net/openvpn3/debian [DISTRIBUTION] main\" | tee /etc/apt/sources.list.d/openvpn3.list Replace [DISTRIBUTION] with your specific Linux distribution release name (e.g., buster, bullseye, bookworm for Debian; focal, jammy for Ubuntu LTS). 3. Install OpenVPN 3 Linux: \u00b6 Update your package list and install OpenVPN 3 Linux: sudo apt update sudo apt install openvpn3 Using OpenVPN 3 Linux \u00b6 Start a One-Shot Configuration Profile: \u00b6 openvpn3 session-start --config [config-file.ovpn] Import a Configuration File for Re-Use: \u00b6 openvpn3 config-import --config [config-file.ovpn] Start a New VPN Session: \u00b6 openvpn3 session-start --config [config-file.ovpn] Manage a Running VPN Session: \u00b6 Manage sessions with the openvpn3 command. For example, to disconnect: openvpn3 sessions-list openvpn3 session-manage --session-path [session-path] --disconnect Further Information \u00b6 For comprehensive instructions, visit the OpenVPN 3 Linux project page and its release notes. ARM64/aarch64 architectures are in tech-preview, and feedback from users on this platform is encouraged. Fedora Copr repository instructions are available separately for Fedora and Red Hat Enterprise Linux users. Note that some architectures may have only tech-preview support or be available in specific repositories. This guide provides a general overview. Always refer to the latest official OpenVPN 3 Linux documentation and your distribution's specific guidelines for up-to-date information. ```","title":"OpenVPN"},{"location":"engineering/aws/openvpn/#installing-the-openvpn-connect-client-on-windows","text":"","title":"Installing the OpenVPN Connect Client on Windows"},{"location":"engineering/aws/openvpn/#download-the-installer","text":"Visit the download link provided: OpenVPN Connect v3 . Click on \"Download OpenVPN Connect v3\".","title":"Download the Installer"},{"location":"engineering/aws/openvpn/#run-the-installer","text":"Locate the .msi file you downloaded and double-click it to start the installation process.","title":"Run the Installer"},{"location":"engineering/aws/openvpn/#installation-assistant","text":"Follow the instructions in the installation wizard. This usually includes agreeing to the terms of the license agreement and selecting the destination folder for installation.","title":"Installation Assistant"},{"location":"engineering/aws/openvpn/#finish-installation","text":"Once the wizard is finished, click \"Finish\" to complete the installation. More details available at OpenVPN Connect Client for Windows .","title":"Finish Installation"},{"location":"engineering/aws/openvpn/#using-the-openvpn-connect-client-to-connect-to-a-vpn","text":"","title":"Using the OpenVPN Connect Client to connect to a VPN"},{"location":"engineering/aws/openvpn/#launch-openvpn-connect-client","text":"Look for OpenVPN Connect in the Windows Start menu and open it.","title":"Launch OpenVPN Connect Client"},{"location":"engineering/aws/openvpn/#import-vpn-profile","text":"On the main interface, click on \"Import Profile\". You have two options: URL : Enter the URL provided by your VPN administrator. FILE : Drag and drop the .ovpn file into the window or click \"BROWSE\" to locate and select the file on your computer. Note: Import the .ovpn files generated for Navigator's DEV and PROD connections here.","title":"Import VPN Profile"},{"location":"engineering/aws/openvpn/#connect-to-the-vpn","text":"Enter your username and password provided by the VPN administrator. Click \"Connect\" to establish the connection to the VPN server.","title":"Connect to the VPN"},{"location":"engineering/aws/openvpn/#connection-verification","text":"Verify that the connection is successful.","title":"Connection Verification"},{"location":"engineering/aws/openvpn/#using-the-vpn","text":"With the VPN connected, all your internet activities will go through the VPN. To disconnect, open OpenVPN Connect and click Disconnect.","title":"Using the VPN"},{"location":"engineering/aws/openvpn/#profile-management","text":"Manage multiple VPN profiles within the app. To switch between profiles, disconnect from the current VPN and repeat the connection process with the desired profile.","title":"Profile Management"},{"location":"engineering/aws/openvpn/#getting-started-with-openvpn-3-linux","text":"OpenVPN 3 Linux is a modern client using the OpenVPN 3 Core Library. It enables unprivileged users to initiate and manage VPN sessions through D-Bus, enhancing privilege separation. This client is optimized for contemporary Linux distributions, but it's compatible with any platform that supports D-Bus.","title":"Getting Started with OpenVPN 3 Linux"},{"location":"engineering/aws/openvpn/#pre-built-packages","text":"OpenVPN 3 Linux v21 offers pre-built packages for various distributions. For production environments, opt for stable releases. Development or beta releases provide more frequent updates.","title":"Pre-built Packages"},{"location":"engineering/aws/openvpn/#supported-distributions","text":"Debian (Buster, Bullseye, Bookworm) Fedora (37, 38, Rawhide) Red Hat Enterprise Linux / CentOS (7, 8, 9) Ubuntu LTS (Focal, Jammy) Ubuntu (Kinetic, Lunar, Mantic)","title":"Supported Distributions"},{"location":"engineering/aws/openvpn/#installation-instructions","text":"","title":"Installation Instructions"},{"location":"engineering/aws/openvpn/#1-add-the-repository","text":"For Debian / Ubuntu, start by installing necessary packages: sudo apt install apt-transport-https curl","title":"1. Add the Repository:"},{"location":"engineering/aws/openvpn/#2-retrieve-and-add-the-package-signing-key","text":"Create a directory for the keyrings, if needed: mkdir -p /etc/apt/keyrings Download and add the OpenVPN Inc package signing key: curl -sSfL https://packages.openvpn.net/packages-repo.gpg > /etc/apt/keyrings/openvpn.asc Add the OpenVPN repository to your system's source list: echo \"deb [signed-by=/etc/apt/keyrings/openvpn.asc] https://packages.openvpn.net/openvpn3/debian [DISTRIBUTION] main\" | tee /etc/apt/sources.list.d/openvpn3.list Replace [DISTRIBUTION] with your specific Linux distribution release name (e.g., buster, bullseye, bookworm for Debian; focal, jammy for Ubuntu LTS).","title":"2. Retrieve and Add the Package Signing Key:"},{"location":"engineering/aws/openvpn/#3-install-openvpn-3-linux","text":"Update your package list and install OpenVPN 3 Linux: sudo apt update sudo apt install openvpn3","title":"3. Install OpenVPN 3 Linux:"},{"location":"engineering/aws/openvpn/#using-openvpn-3-linux","text":"","title":"Using OpenVPN 3 Linux"},{"location":"engineering/aws/openvpn/#start-a-one-shot-configuration-profile","text":"openvpn3 session-start --config [config-file.ovpn]","title":"Start a One-Shot Configuration Profile:"},{"location":"engineering/aws/openvpn/#import-a-configuration-file-for-re-use","text":"openvpn3 config-import --config [config-file.ovpn]","title":"Import a Configuration File for Re-Use:"},{"location":"engineering/aws/openvpn/#start-a-new-vpn-session","text":"openvpn3 session-start --config [config-file.ovpn]","title":"Start a New VPN Session:"},{"location":"engineering/aws/openvpn/#manage-a-running-vpn-session","text":"Manage sessions with the openvpn3 command. For example, to disconnect: openvpn3 sessions-list openvpn3 session-manage --session-path [session-path] --disconnect","title":"Manage a Running VPN Session:"},{"location":"engineering/aws/openvpn/#further-information","text":"For comprehensive instructions, visit the OpenVPN 3 Linux project page and its release notes. ARM64/aarch64 architectures are in tech-preview, and feedback from users on this platform is encouraged. Fedora Copr repository instructions are available separately for Fedora and Red Hat Enterprise Linux users. Note that some architectures may have only tech-preview support or be available in specific repositories. This guide provides a general overview. Always refer to the latest official OpenVPN 3 Linux documentation and your distribution's specific guidelines for up-to-date information. ```","title":"Further Information"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/","text":"# Kubernetes Commands Usage Guide - Navigator Cluster This manual provides detailed instructions on how to use essential Kubernetes commands (`kubectl`) for managing and observing pods in different namespaces, as well as executing commands and tasks within the pods. ## Prerequisites - `kubectl` installed and configured to interact with your Kubernetes cluster. - Access to the Kubernetes cluster where the pods and services are running. ## Retrieving Pod Information ### Listing Pods in a Namespace To get a list of all the pods in a specific namespace: ```bash kubectl -n internal get pods Replace internal with the desired namespace name. Describing a Pod \u00b6 To view the detailed description of a specific pod: kubectl -n internal describe pod logstash-logstash-0 Replace logstash-logstash-0 with the desired pod name. Viewing Pod Logs \u00b6 To view the logs of a specific pod: kubectl -n internal logs dataintegrator-worker-production-d6754d94f-s68px Replace dataintegrator-worker-production-d6754d94f-s68px with the desired pod name. Interacting with Pods \u00b6 Connecting to a Pod \u00b6 To connect to a pod and open an interactive shell inside it: kubectl exec --stdin --tty dataintegrator-worker-production-66ff9d7675-zlvts -n production -- /bin/bash Replace dataintegrator-worker-production-66ff9d7675-zlvts with the desired pod name. Executing Tasks Inside a Worker \u00b6 To run a specific task inside a pod: Connect to the pod: bash kubectl exec --stdin --tty dataintegrator-worker-dev-84cc998454-9d49w -n dev -- /bin/bash Once inside the pod, execute the command for the task: bash di --program=epson --task=epson_store_ranking --debug \u2014no-worker Exporting Logs to a File \u00b6 To export the logs of a pod to a file on your local system: For a pod in the production namespace: bash kubectl -n production logs dt-xfinity-ltm-transactions-cj-28301370-fvd22 > log_ltm_trans.txt For a pod in the dev namespace: bash kubectl -n dev logs dataintegrator-dev-c8f6b9fd6-99g2r > Log_dataintegrator-dev-99g2r.txt For another pod in dev : bash kubectl -n dev logs dataintegrator-worker-production-668486f477-8s2hz > epson_store_ranking01.txt Additional Notes \u00b6 Replace pod and namespace names with those corresponding to your specific environment and needs. Ensure you have the appropriate permissions to execute these commands in the Kubernetes cluster. Refer to the official kubectl documentation for more details and advanced options of these commands. This guide provides you with the basic tools for managing and observing pods in Kubernetes, as well as interacting with them and exporting their logs for further analysis. ```","title":"Kubernetes"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#describing-a-pod","text":"To view the detailed description of a specific pod: kubectl -n internal describe pod logstash-logstash-0 Replace logstash-logstash-0 with the desired pod name.","title":"Describing a Pod"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#viewing-pod-logs","text":"To view the logs of a specific pod: kubectl -n internal logs dataintegrator-worker-production-d6754d94f-s68px Replace dataintegrator-worker-production-d6754d94f-s68px with the desired pod name.","title":"Viewing Pod Logs"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#interacting-with-pods","text":"","title":"Interacting with Pods"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#connecting-to-a-pod","text":"To connect to a pod and open an interactive shell inside it: kubectl exec --stdin --tty dataintegrator-worker-production-66ff9d7675-zlvts -n production -- /bin/bash Replace dataintegrator-worker-production-66ff9d7675-zlvts with the desired pod name.","title":"Connecting to a Pod"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#executing-tasks-inside-a-worker","text":"To run a specific task inside a pod: Connect to the pod: bash kubectl exec --stdin --tty dataintegrator-worker-dev-84cc998454-9d49w -n dev -- /bin/bash Once inside the pod, execute the command for the task: bash di --program=epson --task=epson_store_ranking --debug \u2014no-worker","title":"Executing Tasks Inside a Worker"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#exporting-logs-to-a-file","text":"To export the logs of a pod to a file on your local system: For a pod in the production namespace: bash kubectl -n production logs dt-xfinity-ltm-transactions-cj-28301370-fvd22 > log_ltm_trans.txt For a pod in the dev namespace: bash kubectl -n dev logs dataintegrator-dev-c8f6b9fd6-99g2r > Log_dataintegrator-dev-99g2r.txt For another pod in dev : bash kubectl -n dev logs dataintegrator-worker-production-668486f477-8s2hz > epson_store_ranking01.txt","title":"Exporting Logs to a File"},{"location":"engineering/aws/cloud_plaftorm/Kubernetes/#additional-notes","text":"Replace pod and namespace names with those corresponding to your specific environment and needs. Ensure you have the appropriate permissions to execute these commands in the Kubernetes cluster. Refer to the official kubectl documentation for more details and advanced options of these commands. This guide provides you with the basic tools for managing and observing pods in Kubernetes, as well as interacting with them and exporting their logs for further analysis. ```","title":"Additional Notes"},{"location":"engineering/aws/database/Respaldar_Restaurar_Tablas/","text":"Respaldo de Tablas en Base de Datos en Navigator \u00b6 Este manual proporciona los pasos para realizar el respaldo y restauraci\u00f3n de tablas en la base de datos de Navigator. Conexi\u00f3n a EC2 \u00b6 Conexi\u00f3n SSH al servidor EC2 Con\u00e9ctate al servidor EC2 de producci\u00f3n utilizando SSH: bash ssh usuario@10.0.22.233 Respaldo de Tablas \u00b6 Realizar el Respaldo Una vez conectado, ejecuta el siguiente comando para respaldar una tabla espec\u00edfica: bash pg_dump -h navigator-production-tf.cngboma1n4ol.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W --format=c -d navigator_production -t schema.tabla > bk_schemas_production.backup Para respaldar varias tablas (ejemplo para Hisense): bash pg_dump -h navigator-production-tf.cngboma1n4ol.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W --format=c -d navigator_production -t hisense.products -t hisense.inventory -t hisense.all_products -t hisense.form_data > bk_schemas_production.backup Ingreso de Contrase\u00f1a Ingresa la contrase\u00f1a de la base de datos cuando se solicite. Descarga del Esquema Descarga el esquema de la base de datos a tu m\u00e1quina local. Nota: Puedes usar screen para dejar la terminal corriendo en segundo plano. Restauraci\u00f3n de Tablas \u00b6 Preparaci\u00f3n para la Restauraci\u00f3n Antes de restaurar, si es necesario, elimina las tablas existentes: sql DROP TABLE IF EXISTS schema.tabla; O para m\u00faltiples tablas (ejemplo para Hisense): sql DROP TABLE IF EXISTS hisense.form_data; DROP TABLE IF EXISTS hisense.inventory; DROP TABLE IF EXISTS hisense.all_products; Realizar la Restauraci\u00f3n Ejecuta el siguiente comando para restaurar las tablas: bash pg_restore -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -d navigator_dev -v bk_schemas_hisense_production.backup Fin del tutorial.","title":"Schemes and tables"},{"location":"engineering/aws/database/Respaldar_Restaurar_Tablas/#respaldo-de-tablas-en-base-de-datos-en-navigator","text":"Este manual proporciona los pasos para realizar el respaldo y restauraci\u00f3n de tablas en la base de datos de Navigator.","title":"Respaldo de Tablas en Base de Datos en Navigator"},{"location":"engineering/aws/database/Respaldar_Restaurar_Tablas/#conexion-a-ec2","text":"Conexi\u00f3n SSH al servidor EC2 Con\u00e9ctate al servidor EC2 de producci\u00f3n utilizando SSH: bash ssh usuario@10.0.22.233","title":"Conexi\u00f3n a EC2"},{"location":"engineering/aws/database/Respaldar_Restaurar_Tablas/#respaldo-de-tablas","text":"Realizar el Respaldo Una vez conectado, ejecuta el siguiente comando para respaldar una tabla espec\u00edfica: bash pg_dump -h navigator-production-tf.cngboma1n4ol.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W --format=c -d navigator_production -t schema.tabla > bk_schemas_production.backup Para respaldar varias tablas (ejemplo para Hisense): bash pg_dump -h navigator-production-tf.cngboma1n4ol.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W --format=c -d navigator_production -t hisense.products -t hisense.inventory -t hisense.all_products -t hisense.form_data > bk_schemas_production.backup Ingreso de Contrase\u00f1a Ingresa la contrase\u00f1a de la base de datos cuando se solicite. Descarga del Esquema Descarga el esquema de la base de datos a tu m\u00e1quina local. Nota: Puedes usar screen para dejar la terminal corriendo en segundo plano.","title":"Respaldo de Tablas"},{"location":"engineering/aws/database/Respaldar_Restaurar_Tablas/#restauracion-de-tablas","text":"Preparaci\u00f3n para la Restauraci\u00f3n Antes de restaurar, si es necesario, elimina las tablas existentes: sql DROP TABLE IF EXISTS schema.tabla; O para m\u00faltiples tablas (ejemplo para Hisense): sql DROP TABLE IF EXISTS hisense.form_data; DROP TABLE IF EXISTS hisense.inventory; DROP TABLE IF EXISTS hisense.all_products; Realizar la Restauraci\u00f3n Ejecuta el siguiente comando para restaurar las tablas: bash pg_restore -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -d navigator_dev -v bk_schemas_hisense_production.backup Fin del tutorial.","title":"Restauraci\u00f3n de Tablas"},{"location":"engineering/aws/database/backup_schema_Troc/","text":"Respaldo de Schema \u00b6 1. Conexi\u00f3n a EC2 \u00b6 Paso 1 : Conectarnos al EC2 de Prod. ssh usuario@10.0.22.233 2. Respaldo del Esquema \u00b6 Paso 2 : Una vez conectados, ejecutamos el siguiente comando para realizar el respaldo del esquema: pg_dump -h navigator-production-tf.cngboma1n4ol.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W --schema-only -d navigator_production > bk_schemas_production.sql Paso 3 : Colocamos la contrase\u00f1a de la base de datos cuando se solicite. 3. Descarga del Esquema \u00b6 Paso 4 : Nos bajamos al esquema. Nota : Podemos usar Screen en algunas ocasiones para dejar la terminal corriendo en segundo plano.","title":"Backups"},{"location":"engineering/aws/database/backup_schema_Troc/#respaldo-de-schema","text":"","title":"Respaldo de Schema"},{"location":"engineering/aws/database/backup_schema_Troc/#1-conexion-a-ec2","text":"Paso 1 : Conectarnos al EC2 de Prod. ssh usuario@10.0.22.233","title":"1. Conexi\u00f3n a EC2"},{"location":"engineering/aws/database/backup_schema_Troc/#2-respaldo-del-esquema","text":"Paso 2 : Una vez conectados, ejecutamos el siguiente comando para realizar el respaldo del esquema: pg_dump -h navigator-production-tf.cngboma1n4ol.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W --schema-only -d navigator_production > bk_schemas_production.sql Paso 3 : Colocamos la contrase\u00f1a de la base de datos cuando se solicite.","title":"2. Respaldo del Esquema"},{"location":"engineering/aws/database/backup_schema_Troc/#3-descarga-del-esquema","text":"Paso 4 : Nos bajamos al esquema. Nota : Podemos usar Screen en algunas ocasiones para dejar la terminal corriendo en segundo plano.","title":"3. Descarga del Esquema"},{"location":"engineering/aws/database/extensiones_requeridas_pg/","text":"Extension Installation Manual \u00b6 To enable additional functionality in PostgreSQL, the following extensions are required: \u00b6 CREATE EXTENSION btree_gin; CREATE EXTENSION btree_gist; CREATE EXTENSION hstore; CREATE EXTENSION tablefunc; CREATE EXTENSION \"uuid-ossp\"; CREATE EXTENSION fuzzystrmatch; CREATE EXTENSION pg_trgm; These extensions provide various capabilities, such as additional indexes, semi-structured data handling, and UUID generation. \u00b6 Indices that must be created \u00b6 In table troc.troc_employees \u00b6 To improve the performance of queries involving email searches, the following indexes have been created: \u00b6 CREATE INDEX trgm_troc_email_idx ON troc.troc_employees USING gin (corporate_email gin_trgm_ops); CREATE INDEX trgm_troc_old_email_idx ON troc.troc_employees USING gin (old_corporate_email gin_trgm_ops); In table epson.users_raw \u00b6 To optimize text search queries in the email column, the following index has been created: \u00b6 CREATE INDEX trgm_user_idx ON epson.users_raw USING gin (email gin_trgm_ops); These indexes improve the speed of queries that involve text searches on the specified columns. \u00b6","title":"Create PG Extensions"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#extension-installation-manual","text":"","title":"Extension Installation Manual"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#to-enable-additional-functionality-in-postgresql-the-following-extensions-are-required","text":"CREATE EXTENSION btree_gin; CREATE EXTENSION btree_gist; CREATE EXTENSION hstore; CREATE EXTENSION tablefunc; CREATE EXTENSION \"uuid-ossp\"; CREATE EXTENSION fuzzystrmatch; CREATE EXTENSION pg_trgm;","title":"To enable additional functionality in PostgreSQL, the following extensions are required:"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#these-extensions-provide-various-capabilities-such-as-additional-indexes-semi-structured-data-handling-and-uuid-generation","text":"","title":"These extensions provide various capabilities, such as additional indexes, semi-structured data handling, and UUID generation."},{"location":"engineering/aws/database/extensiones_requeridas_pg/#indices-that-must-be-created","text":"","title":"Indices that must be created"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#in-table-troctroc_employees","text":"","title":"In table troc.troc_employees"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#to-improve-the-performance-of-queries-involving-email-searches-the-following-indexes-have-been-created","text":"CREATE INDEX trgm_troc_email_idx ON troc.troc_employees USING gin (corporate_email gin_trgm_ops); CREATE INDEX trgm_troc_old_email_idx ON troc.troc_employees USING gin (old_corporate_email gin_trgm_ops);","title":"To improve the performance of queries involving email searches, the following indexes have been created:"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#in-table-epsonusers_raw","text":"","title":"In table epson.users_raw"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#to-optimize-text-search-queries-in-the-email-column-the-following-index-has-been-created","text":"CREATE INDEX trgm_user_idx ON epson.users_raw USING gin (email gin_trgm_ops);","title":"To optimize text search queries in the email column, the following index has been created:"},{"location":"engineering/aws/database/extensiones_requeridas_pg/#these-indexes-improve-the-speed-of-queries-that-involve-text-searches-on-the-specified-columns","text":"","title":"These indexes improve the speed of queries that involve text searches on the specified columns."},{"location":"engineering/aws/database/rdsconfig/","text":"RDS Master-Slave Navigator \u00b6 1. Introduction \u00b6 Objective \u00b6 This documentation provides an overview of the configuration and management of our databases in RDS, with a master-slave architecture. Scope \u00b6 Covers the configurations and management procedures of the master and slave databases. 2. General Description \u00b6 Architecture \u00b6 Details the master-slave structure of the databases, where the master database handles I/O operations and the slave database is configured for read-only access. 3. Configuration Details \u00b6 Master Configuration \u00b6 max_connections = 1000 shared_buffers = 2048000 effective_cache_size = 2147483647 maintenance_work_mem = 512000 checkpoint_completion_target = 0.9 wal_buffers = 16000 default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 50 work_mem = 64000 min_wal_size = 1024 max_wal_size = 4000 Replica Configuration \u00b6 max_connections = 1000 shared_buffers = 2048000 effective_cache_size = 2147483647 maintenance_work_mem = 512000 checkpoint_completion_target = 0.9 wal_buffers = 16000 default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 50 work_mem = 64000 min_wal_size = 1024 max_wal_size = 4000","title":"RDS"},{"location":"engineering/aws/database/rdsconfig/#rds-master-slave-navigator","text":"","title":"RDS Master-Slave Navigator"},{"location":"engineering/aws/database/rdsconfig/#1-introduction","text":"","title":"1. Introduction"},{"location":"engineering/aws/database/rdsconfig/#objective","text":"This documentation provides an overview of the configuration and management of our databases in RDS, with a master-slave architecture.","title":"Objective"},{"location":"engineering/aws/database/rdsconfig/#scope","text":"Covers the configurations and management procedures of the master and slave databases.","title":"Scope"},{"location":"engineering/aws/database/rdsconfig/#2-general-description","text":"","title":"2. General Description"},{"location":"engineering/aws/database/rdsconfig/#architecture","text":"Details the master-slave structure of the databases, where the master database handles I/O operations and the slave database is configured for read-only access.","title":"Architecture"},{"location":"engineering/aws/database/rdsconfig/#3-configuration-details","text":"","title":"3. Configuration Details"},{"location":"engineering/aws/database/rdsconfig/#master-configuration","text":"max_connections = 1000 shared_buffers = 2048000 effective_cache_size = 2147483647 maintenance_work_mem = 512000 checkpoint_completion_target = 0.9 wal_buffers = 16000 default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 50 work_mem = 64000 min_wal_size = 1024 max_wal_size = 4000","title":"Master Configuration"},{"location":"engineering/aws/database/rdsconfig/#replica-configuration","text":"max_connections = 1000 shared_buffers = 2048000 effective_cache_size = 2147483647 maintenance_work_mem = 512000 checkpoint_completion_target = 0.9 wal_buffers = 16000 default_statistics_target = 100 random_page_cost = 1.1 effective_io_concurrency = 50 work_mem = 64000 min_wal_size = 1024 max_wal_size = 4000","title":"Replica Configuration"},{"location":"engineering/aws/database/restaurar_db/","text":"Manipulacion de Bases de Datos Navagitor -- \u00b6 Resturar una Base de datos. 1 - Descargamos el dump en produccion (ftp) 2 - Subimos el archivo FTP en DEV - 10.0.22.66 - usuario - puerto: 22 3 - Una vez tenemos el dump en EC2 de DEV procedemos a restuarlo en en el ambiente a utilizar. en Produccion. o en DEV si vamos a restuarala. o en Stating. Nota: usar Screen para evitar tener la cosola de activa por mucho tiempo. 1 - DROPEAR BD dropdb -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W navigator_staging ( o la que aplique) opcional: con este query podemos desconectar las sesioens que esten usando al DB. SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'temp' AND pid <> pg_backend_pid() Segun aplique: 2 - CREAR BD createdb -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W navigator_staging ( o la que aplique) 3 - RESTAURAR UN BACKUO EN BD QUE HEMOS CREADO - PARA ESTE EJEMPLO TEMP pg_restore -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -d navigator_staging -v ARCHIVO_A_RESTURAR.backup","title":"Restore"},{"location":"engineering/aws/database/restaurar_db/#manipulacion-de-bases-de-datos-navagitor-","text":"Resturar una Base de datos. 1 - Descargamos el dump en produccion (ftp) 2 - Subimos el archivo FTP en DEV - 10.0.22.66 - usuario - puerto: 22 3 - Una vez tenemos el dump en EC2 de DEV procedemos a restuarlo en en el ambiente a utilizar. en Produccion. o en DEV si vamos a restuarala. o en Stating. Nota: usar Screen para evitar tener la cosola de activa por mucho tiempo. 1 - DROPEAR BD dropdb -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W navigator_staging ( o la que aplique) opcional: con este query podemos desconectar las sesioens que esten usando al DB. SELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = 'temp' AND pid <> pg_backend_pid() Segun aplique: 2 - CREAR BD createdb -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -W navigator_staging ( o la que aplique) 3 - RESTAURAR UN BACKUO EN BD QUE HEMOS CREADO - PARA ESTE EJEMPLO TEMP pg_restore -h navigator-staging-tf.cp0qwiwsxfog.us-east-2.rds.amazonaws.com -p 5432 -U troc_pgdata -d navigator_staging -v ARCHIVO_A_RESTURAR.backup","title":"Manipulacion de Bases de Datos Navagitor --"},{"location":"engineering/gcp/","text":"Google Cloud Platform \u00b6","title":"Home"},{"location":"engineering/gcp/#google-cloud-platform","text":"","title":"Google Cloud Platform"},{"location":"engineering/gcp/access/","text":"Access to the Platform \u00b6 This guide provides the necessary information to access the Navigator Platform on the GCP (Google Cloud Platform) side. Please ensure you have everything needed from the list below. Requirements \u00b6 OpenVPN Client : Download it from here . OpenVPN Access : During the onboarding process, you should have been provided with a file (e.g., username.ovpn ). Make sure you import this file into the VPN client. gcloud CLI: In case you want to access the Kuberneetes cluster. Download it from here gke-gcloud-auth-plugin : Kubectl requires it to use GCP credentials for accessing the cluster. Please check the latest documentation for installation. As of the current date, it involves executing the following command: gcloud components install gke-gcloud-auth-plugin Development Environment \u00b6 Connecting to the Cluster (a.k.a troc-cluster-dev ) \u00b6 You must have the gcloud CLI installed and configured with your username. Set the context with the cluster using the following command: $ gcloud container clusters get-credentials navigator-platform --region us-west1 --project troc-cluster-dev","title":"Access"},{"location":"engineering/gcp/access/#access-to-the-platform","text":"This guide provides the necessary information to access the Navigator Platform on the GCP (Google Cloud Platform) side. Please ensure you have everything needed from the list below.","title":"Access to the Platform"},{"location":"engineering/gcp/access/#requirements","text":"OpenVPN Client : Download it from here . OpenVPN Access : During the onboarding process, you should have been provided with a file (e.g., username.ovpn ). Make sure you import this file into the VPN client. gcloud CLI: In case you want to access the Kuberneetes cluster. Download it from here gke-gcloud-auth-plugin : Kubectl requires it to use GCP credentials for accessing the cluster. Please check the latest documentation for installation. As of the current date, it involves executing the following command: gcloud components install gke-gcloud-auth-plugin","title":"Requirements"},{"location":"engineering/gcp/access/#development-environment","text":"","title":"Development Environment"},{"location":"engineering/gcp/access/#connecting-to-the-cluster-aka-troc-cluster-dev","text":"You must have the gcloud CLI installed and configured with your username. Set the context with the cluster using the following command: $ gcloud container clusters get-credentials navigator-platform --region us-west1 --project troc-cluster-dev","title":"Connecting to the Cluster (a.k.a troc-cluster-dev)"},{"location":"engineering/gcp/openvpn/","text":"OpenVPN \u00b6 A VPN server allows us to connect securely to the platform. We currently have two VPN servers in GCP under the following accounts: troc-cluster-dev troc-cluster-prd Each of these VPN servers enables us to connect to the platform in their respective environments. We are currently using OpenVPN-installer to manage the OpenVPN servers. Creating and Deleting Users in the VPN \u00b6 To create or delete a user: Connect to the VPN server: ssh <username>@<ip-address> . To obtain the IP address, check the Google Cloud UI. Use the OpenVPN script to manage user creation or deletion. It is very intuitive; here is an example: ```shell mogaal@openvpn:~$ sudo /srv/openvpn-install.sh Welcome to OpenVPN-install! The git repository is available at: https://github.com/angristan/openvpn-install It looks like OpenVPN is already installed. What do you want to do? 1) Add a new user 2) Revoke existing user 3) Remove OpenVPN 4) Exit Select an option [1-4]: 1 Tell me a name for the client. The name must consist of alphanumeric character. It may also include an underscore or a dash. Client name: oslan Do you want to protect the configuration file with a password? (e.g. encrypt the private key with a password) 1) Add a passwordless client 2) Use a password for the client Select an option [1-2]: 1 Using SSL: openssl OpenSSL 3.0.8 7 Feb 2023 (Library: OpenSSL 3.0.8 7 Feb 2023) Using Easy-RSA configuration: /etc/openvpn/easy-rsa/vars The preferred location for 'vars' is within the PKI folder. To silence this message move your 'vars' file to your PKI or declare your 'vars' file with option: --vars= Notice \u00b6 Keypair and certificate request completed. Your files are: req: /etc/openvpn/easy-rsa/pki/reqs/oslan.req key: /etc/openvpn/easy-rsa/pki/private/oslan.key Using configuration from /etc/openvpn/easy-rsa/pki/401e8d65/temp.d30a61f3 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'oslan' Certificate is to be certified until Mar 10 12:02:09 2026 GMT (825 days) Write out database with 1 new entries Data Base Updated Notice \u00b6 Certificate created at: * /etc/openvpn/easy-rsa/pki/issued/oslan.crt Notice \u00b6 Inline file created: * /etc/openvpn/easy-rsa/pki/inline/oslan.inline Client oslan added. The configuration file has been written to /home/mogaal/oslan.ovpn. Download the .ovpn file and import it in your OpenVPN client. mogaal@openvpn:~$ ``` Share the username.ovpn file with the user and ask them to install it on their machine.","title":"OpenVPN"},{"location":"engineering/gcp/openvpn/#openvpn","text":"A VPN server allows us to connect securely to the platform. We currently have two VPN servers in GCP under the following accounts: troc-cluster-dev troc-cluster-prd Each of these VPN servers enables us to connect to the platform in their respective environments. We are currently using OpenVPN-installer to manage the OpenVPN servers.","title":"OpenVPN"},{"location":"engineering/gcp/openvpn/#creating-and-deleting-users-in-the-vpn","text":"To create or delete a user: Connect to the VPN server: ssh <username>@<ip-address> . To obtain the IP address, check the Google Cloud UI. Use the OpenVPN script to manage user creation or deletion. It is very intuitive; here is an example: ```shell mogaal@openvpn:~$ sudo /srv/openvpn-install.sh Welcome to OpenVPN-install! The git repository is available at: https://github.com/angristan/openvpn-install It looks like OpenVPN is already installed. What do you want to do? 1) Add a new user 2) Revoke existing user 3) Remove OpenVPN 4) Exit Select an option [1-4]: 1 Tell me a name for the client. The name must consist of alphanumeric character. It may also include an underscore or a dash. Client name: oslan Do you want to protect the configuration file with a password? (e.g. encrypt the private key with a password) 1) Add a passwordless client 2) Use a password for the client Select an option [1-2]: 1 Using SSL: openssl OpenSSL 3.0.8 7 Feb 2023 (Library: OpenSSL 3.0.8 7 Feb 2023) Using Easy-RSA configuration: /etc/openvpn/easy-rsa/vars The preferred location for 'vars' is within the PKI folder. To silence this message move your 'vars' file to your PKI or declare your 'vars' file with option: --vars=","title":"Creating and Deleting Users in the VPN"},{"location":"engineering/gcp/openvpn/#notice","text":"Keypair and certificate request completed. Your files are: req: /etc/openvpn/easy-rsa/pki/reqs/oslan.req key: /etc/openvpn/easy-rsa/pki/private/oslan.key Using configuration from /etc/openvpn/easy-rsa/pki/401e8d65/temp.d30a61f3 Check that the request matches the signature Signature ok The Subject's Distinguished Name is as follows commonName :ASN.1 12:'oslan' Certificate is to be certified until Mar 10 12:02:09 2026 GMT (825 days) Write out database with 1 new entries Data Base Updated","title":"Notice"},{"location":"engineering/gcp/openvpn/#notice_1","text":"Certificate created at: * /etc/openvpn/easy-rsa/pki/issued/oslan.crt","title":"Notice"},{"location":"engineering/gcp/openvpn/#notice_2","text":"Inline file created: * /etc/openvpn/easy-rsa/pki/inline/oslan.inline Client oslan added. The configuration file has been written to /home/mogaal/oslan.ovpn. Download the .ovpn file and import it in your OpenVPN client. mogaal@openvpn:~$ ``` Share the username.ovpn file with the user and ask them to install it on their machine.","title":"Notice"},{"location":"engineering/gcp/projects/","text":"Creating google projects on GPC \u00b6 This guide provides the necessary information to create a new GCP project. The process consist in raise a change, ask for an approval from a platform member and creation is going to happens automatically. Requirements \u00b6 Before you begin, ensure you have access to the cloud platform repository. It is the only prerequisite. If you don't have access, please contact any member of the infrastructure team. Step 1: Open cloud platform repo \u00b6 Go to Github and open the cloud platform repo , everything can be done directly online, and there is no need to use the Git CLI at all. Open file Step 2: Edit the main.tf file \u00b6 Click on the pencil button to edit the file. Edit file Go to the end of the file Add your new google project block (See the example below) Replace my_new_project and my-new-project with a descriptive project name Adjust activate_apis apis with the GCP services you will need for this project. This is an example: module \"my_new_project\" { source = \"terraform-google-modules/project-factory/google\" version = \"14.4.0\" name = \"my-new-project\" org_id = local.organization_id billing_account = local.billing_account default_service_account = \"deprivilege\" activate_apis = [ \"logging.googleapis.com\", \"monitoring.googleapis.com\", \"compute.googleapis.com\", \"dns.googleapis.com\", \"container.googleapis.com\", \"iamcredentials.googleapis.com\", \"cloudresourcemanager.googleapis.com\", \"iam.googleapis.com\", \"alloydb.googleapis.com\", \"artifactregistry.googleapis.com\", \"file.googleapis.com\", \"memcache.googleapis.com\", \"redis.googleapis.com\", \"servicenetworking.googleapis.com\", ] } Step 3: Commit the changes and and push a new branch \u00b6 Commit the changes by pressing the \"Commit Changes\" button in the top right corner: commit changes Create a new branch by clicking the \"Propose new changes\" button. commit changes Add a title and a description explaining why you need a new project. The clearer your explanation, the quicker the team can approve it. PR created Step 4: Ask for approval \u00b6 The final step is to ask a member of the infrastructure/platform team for a quick review and approval. Steps for the Infrastructure Team \u00b6 Note The entire process is completely automated by Atlantis; it will apply the changes without requiring any manual intervention from you, other than reviewing the PR. Review Atlantis output : It is important to review the Atlantis output, including the objects to be created. Examine the file contents and the Atlantis comment with the Terraform output Atlantis comment with terraform output Approve the PR and apply if everything looks good : First, approve the PR on GitHub; second, run atlantis apply. After that atlantis will apply the changes and close the PR once everything is ready Atlantis applies and closes the PR","title":"Create Projects"},{"location":"engineering/gcp/projects/#creating-google-projects-on-gpc","text":"This guide provides the necessary information to create a new GCP project. The process consist in raise a change, ask for an approval from a platform member and creation is going to happens automatically.","title":"Creating google projects on GPC"},{"location":"engineering/gcp/projects/#requirements","text":"Before you begin, ensure you have access to the cloud platform repository. It is the only prerequisite. If you don't have access, please contact any member of the infrastructure team.","title":"Requirements"},{"location":"engineering/gcp/projects/#step-1-open-cloud-platform-repo","text":"Go to Github and open the cloud platform repo , everything can be done directly online, and there is no need to use the Git CLI at all. Open file","title":"Step 1: Open cloud platform repo"},{"location":"engineering/gcp/projects/#step-2-edit-the-maintf-file","text":"Click on the pencil button to edit the file. Edit file Go to the end of the file Add your new google project block (See the example below) Replace my_new_project and my-new-project with a descriptive project name Adjust activate_apis apis with the GCP services you will need for this project. This is an example: module \"my_new_project\" { source = \"terraform-google-modules/project-factory/google\" version = \"14.4.0\" name = \"my-new-project\" org_id = local.organization_id billing_account = local.billing_account default_service_account = \"deprivilege\" activate_apis = [ \"logging.googleapis.com\", \"monitoring.googleapis.com\", \"compute.googleapis.com\", \"dns.googleapis.com\", \"container.googleapis.com\", \"iamcredentials.googleapis.com\", \"cloudresourcemanager.googleapis.com\", \"iam.googleapis.com\", \"alloydb.googleapis.com\", \"artifactregistry.googleapis.com\", \"file.googleapis.com\", \"memcache.googleapis.com\", \"redis.googleapis.com\", \"servicenetworking.googleapis.com\", ] }","title":"Step 2: Edit the main.tf file"},{"location":"engineering/gcp/projects/#step-3-commit-the-changes-and-and-push-a-new-branch","text":"Commit the changes by pressing the \"Commit Changes\" button in the top right corner: commit changes Create a new branch by clicking the \"Propose new changes\" button. commit changes Add a title and a description explaining why you need a new project. The clearer your explanation, the quicker the team can approve it. PR created","title":"Step 3: Commit the changes and and push a new branch"},{"location":"engineering/gcp/projects/#step-4-ask-for-approval","text":"The final step is to ask a member of the infrastructure/platform team for a quick review and approval.","title":"Step 4: Ask for approval"},{"location":"engineering/gcp/projects/#steps-for-the-infrastructure-team","text":"Note The entire process is completely automated by Atlantis; it will apply the changes without requiring any manual intervention from you, other than reviewing the PR. Review Atlantis output : It is important to review the Atlantis output, including the objects to be created. Examine the file contents and the Atlantis comment with the Terraform output Atlantis comment with terraform output Approve the PR and apply if everything looks good : First, approve the PR on GitHub; second, run atlantis apply. After that atlantis will apply the changes and close the PR once everything is ready Atlantis applies and closes the PR","title":"Steps for the Infrastructure Team"},{"location":"engineering/gcp/containers/gcp_artifact_registry/","text":"# AWS and Google Cloud Image Management Workflow ## 1. Activate the Virtual Environment Activate your virtual environment: ```bash source .venv/bin/activate 2. Establish AWS Connection \u00b6 Connect to your AWS environment: awsume tr-dev Enter MFA token: **** 3. Clone Images from AWS ECR Repository \u00b6 Locate your image repository and proceed to clone the images in AWS: AWS ECR Console In the active awsume console, establish a connection: Connecting with ECR: \u00b6 Authenticate Docker with AWS ECR: bash aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 553449903150.dkr.ecr.us-east-2.amazonaws.com Download the Image: bash docker pull 553449903150.dkr.ecr.us-east-2.amazonaws.com/dataintegrator-tf:dev Note: When performing docker pull , change the final tag from latest to dev . Uploading to Google Cloud \u00b6 To upload the connection to Google Cloud, first validate with the command docker images . List the downloaded images, in this case corresponding to 8cbdded112a2 , and create a new tag: docker tag 8cbdded112a2 us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-tf:dev Then, push the image to your Artifact Registry repository: docker push us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-tf:dev Using the Data Integrator Worker: \u00b6 Repeat the process for the dataintegrator-worker image: Pull the Image from AWS ECR: bash docker pull 553449903150.dkr.ecr.us-east-2.amazonaws.com/dataintegrator-worker-tf:dev Tag the Image for Google Cloud: bash docker tag 027e72d4613a us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-worker-tf:dev Push to Artifact Registry: bash docker push us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-worker-tf:dev ```","title":"Container Registry (ECR/GCR)"},{"location":"engineering/gcp/containers/gcp_artifact_registry/#2-establish-aws-connection","text":"Connect to your AWS environment: awsume tr-dev Enter MFA token: ****","title":"2. Establish AWS Connection"},{"location":"engineering/gcp/containers/gcp_artifact_registry/#3-clone-images-from-aws-ecr-repository","text":"Locate your image repository and proceed to clone the images in AWS: AWS ECR Console In the active awsume console, establish a connection:","title":"3. Clone Images from AWS ECR Repository"},{"location":"engineering/gcp/containers/gcp_artifact_registry/#connecting-with-ecr","text":"Authenticate Docker with AWS ECR: bash aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 553449903150.dkr.ecr.us-east-2.amazonaws.com Download the Image: bash docker pull 553449903150.dkr.ecr.us-east-2.amazonaws.com/dataintegrator-tf:dev Note: When performing docker pull , change the final tag from latest to dev .","title":"Connecting with ECR:"},{"location":"engineering/gcp/containers/gcp_artifact_registry/#uploading-to-google-cloud","text":"To upload the connection to Google Cloud, first validate with the command docker images . List the downloaded images, in this case corresponding to 8cbdded112a2 , and create a new tag: docker tag 8cbdded112a2 us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-tf:dev Then, push the image to your Artifact Registry repository: docker push us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-tf:dev","title":"Uploading to Google Cloud"},{"location":"engineering/gcp/containers/gcp_artifact_registry/#using-the-data-integrator-worker","text":"Repeat the process for the dataintegrator-worker image: Pull the Image from AWS ECR: bash docker pull 553449903150.dkr.ecr.us-east-2.amazonaws.com/dataintegrator-worker-tf:dev Tag the Image for Google Cloud: bash docker tag 027e72d4613a us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-worker-tf:dev Push to Artifact Registry: bash docker push us-west1-docker.pkg.dev/troc-cluster-dev/navigator-dev/dataintegrator-worker-tf:dev ```","title":"Using the Data Integrator Worker:"},{"location":"engineering/gcp/platform-services/rethinkdb-connection/","text":"RethinkDB \u00b6 RethinkDB is the open-source, scalable database that makes building realtime apps dramatically easier. We have RethinkDB integrated into the Navigator Platform, which is running in GKE and managed through ArgoCD. As of today, it is not exposed to the internet (and it most likely never will be), so in order to connect to it, we must be within the GKE cluster (e.g., using a test pod). Steps to test the connection from any namespace: \u00b6 Ensure that you are conducting the test using a pod inside the Kubernetes cluster. Install the correct driver (for this example, we will use Python). For instructions, see here . Set up a test script to verify the connection. Instructions can be found here in the Usage section. Use the rethinkdb service URL for the cluster connection and the default password URL: rethinkdb-rethinkdb-cluster.rethinkdb.svc.cluster.local Password: rethinkdb Full script to verify the connection: ```python from rethinkdb import r r.connect('rethinkdb-rethinkdb-cluster.rethinkdb.svc.cluster.local', password=\"rethinkdb\").repl() r.db('test').table_create('tv_shows').run() r.table('tv_shows').insert({ 'name': 'Star Trek TNG' }).run() ``` Run this script inside the pod and the connection should be ready to use the rethinkdb driver","title":"Rethinkdb"},{"location":"engineering/gcp/platform-services/rethinkdb-connection/#rethinkdb","text":"RethinkDB is the open-source, scalable database that makes building realtime apps dramatically easier. We have RethinkDB integrated into the Navigator Platform, which is running in GKE and managed through ArgoCD. As of today, it is not exposed to the internet (and it most likely never will be), so in order to connect to it, we must be within the GKE cluster (e.g., using a test pod).","title":"RethinkDB"},{"location":"engineering/gcp/platform-services/rethinkdb-connection/#steps-to-test-the-connection-from-any-namespace","text":"Ensure that you are conducting the test using a pod inside the Kubernetes cluster. Install the correct driver (for this example, we will use Python). For instructions, see here . Set up a test script to verify the connection. Instructions can be found here in the Usage section. Use the rethinkdb service URL for the cluster connection and the default password URL: rethinkdb-rethinkdb-cluster.rethinkdb.svc.cluster.local Password: rethinkdb Full script to verify the connection: ```python from rethinkdb import r r.connect('rethinkdb-rethinkdb-cluster.rethinkdb.svc.cluster.local', password=\"rethinkdb\").repl() r.db('test').table_create('tv_shows').run() r.table('tv_shows').insert({ 'name': 'Star Trek TNG' }).run() ``` Run this script inside the pod and the connection should be ready to use the rethinkdb driver","title":"Steps to test the connection from any namespace:"},{"location":"engineering/gcp/platform-services/scylladb-connection/","text":"ScyllaDB \u00b6 ScyllaDB is an open-source, highly performant, distributed NoSQL database. It's designed to handle large amounts of data with low latency and is compatible with Apache Cassandra, making it a popular choice for real-time big data applications. ScyllaDB is integrated into our Navigator Platform, running in GKE and managed through ArgoCD. It's not exposed to the internet for security reasons. To interact with ScyllaDB, we need to be within the GKE cluster, typically using a test pod. Steps to test the connection from any namespace: \u00b6 First, make sure you are testing using a pod inside the kubernetes cluster Install the correct driver (In this example we will use python) See here Set up a test script to verify the connection See here Use the scylladb service URL for the cluster connection URL: scylladb-client.scylla.svc.cluster.local Full script to verify the connection: ```python from cassandra.cluster import Cluster print(\"starting\") cluster = Cluster(['scylladb-client.scylla.svc.cluster.local']) print(\"connecting\") session = cluster.connect() print(\"connected\") ``` Run this script inside the pod and the connection should be ready to use scylladb using the cassandra driver!","title":"ScyllaDB"},{"location":"engineering/gcp/platform-services/scylladb-connection/#scylladb","text":"ScyllaDB is an open-source, highly performant, distributed NoSQL database. It's designed to handle large amounts of data with low latency and is compatible with Apache Cassandra, making it a popular choice for real-time big data applications. ScyllaDB is integrated into our Navigator Platform, running in GKE and managed through ArgoCD. It's not exposed to the internet for security reasons. To interact with ScyllaDB, we need to be within the GKE cluster, typically using a test pod.","title":"ScyllaDB"},{"location":"engineering/gcp/platform-services/scylladb-connection/#steps-to-test-the-connection-from-any-namespace","text":"First, make sure you are testing using a pod inside the kubernetes cluster Install the correct driver (In this example we will use python) See here Set up a test script to verify the connection See here Use the scylladb service URL for the cluster connection URL: scylladb-client.scylla.svc.cluster.local Full script to verify the connection: ```python from cassandra.cluster import Cluster print(\"starting\") cluster = Cluster(['scylladb-client.scylla.svc.cluster.local']) print(\"connecting\") session = cluster.connect() print(\"connected\") ``` Run this script inside the pod and the connection should be ready to use scylladb using the cassandra driver!","title":"Steps to test the connection from any namespace:"}]}